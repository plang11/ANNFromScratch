---
title: "MNISTNeuralNetwork"
author: "Patrick Lang"
date: "11/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# MNIST Neural Network in Base R

The following code shows how to create a neural network and train it. The NN that will be created has 3 layers:  

* Input Layer - 784 units (28*28=784 pixels per handwritten digit)
* Hidden Layer - 32 units (arbitrarily chosen, can be tweaked)
* Output Layer - 10 units (represents the output classes, numbers from 0-9)

```{r loading packages}

#only packages required are to one-hot encode the label vector, nothing to do with the actual neural network
library(data.table) 
library(mltools)

```

```{r importing data}

#importing data
#make sure you have unzipped the data file and placed it in the "Data" folder for this to work
train <- read.csv("../Data/train.csv")

```

## Data Preprocessing
The first step is to split the images from the label vector.  
Then we must transform our image data into the shape our neural network will expect *(n_pixels, n_examples)*   
The final data preprocessing step is to one hot encode the label vector *(n_examples, 1)* to a *(n_classes, n_examples)* matrix

```{r splitting into X and y}

#reshaping X into shape (n_pixels, n_examples), it was previously (n_examples, n_pixels)
X <- t(as.matrix(train[,-1]))/255
#removing column names
X <- matrix(X, 784, 42000)

#one hot encoding y as there are 10 possible classes (digits) and our output layer will contain 10 units, also reshaping to be (n_classes, n_examples)
#only time a package is used
y <- data.table(train[,1])
y$V1 <- as.factor(y$V1)
y <- one_hot(y, cols = "V1")
y <- t(as.matrix(y))
y <- matrix(y, 10, 42000)

```

## Creating Functions to Build and Train the NN
This next section of code involves creating all of the necessary functions to build and train our neural network. They go as follows:  

* Parameter initialization 
* Forward propogation 
    + Linear forward step
    + Activation functions (sigmoid and ReLU)   
* Loss function 
* Backward propogation 
    + Linear backward step 
    + Activation gradients (sigmoid and ReLU) 
* Parameter updating

### Initializing Parameters
This function takes inputs:  

* n_x --- size of the input layer (n_pixels)  
* n_h --- size of the hidden layer (32)  
* n_y --- size of the output layer (n_classes)

and creates the list *parameters* that contains:  

* W1 --- weight matrix of shape (n_h, n_x)  
* b1 --- bias vector of shape (n_h, 1)  
* W2 --- weight matrix of shape (n_y, n_h)  
* b2 --- bias vector of shape (n_y, 1)  

```{r initializing parameters}

#creating function to initialize weights and biases of neural network
initialize_parameters <- function(n_x, n_h, n_y) {
  
  set.seed(123)
  
  #n_x - size of input layer (n_pixels)
  #n_h - size of hidden layer (n_units)
  #n_y - size of output layer (n_classes)
  
  #randomly initializing weight matrices using uniform distribution and bias vectors as zero vectors
  W1 <- matrix(runif(n_h * n_x), n_h, n_x)*.01
  b1 <- as.vector(rep(0, n_h))
  W2 <- matrix(runif(n_y * n_h), n_y, n_h)*.01
  b2 <- as.vector(rep(0, n_y))
  
  #W1 - weight matrix of shape (n_h, n_x)
  #b1 - bias vector of shape (n_h, 1)
  #W2 - weight matrix of shape (n_y, n_h)
  #b2 - bias vector of shape (n_y, 1)
  
  #combining parameters into a list
  parameters <- list("W1" = W1,
                     "b1" = b1,
                     "W2" = W2,
                     "b2" = b2)
  
  return(parameters)
  
}

```

### Linear Forward Propogation
This function takes inputs:  

* A --- activations from previous layer (or input data) of shape (size of previous layer, n_examples)
* W --- weight matrix of shape (size of current layer, size of previous layer)
* b --- bias vector of shape (size of current layer, 1)  

and returns the list *Z_and_cache* which contains:

* Z --- output of the linear function $Z^{[l]} = W^{[l]}*A^{[l-1]}+b^{[l]}$
* cache --- a list containing the inputs, $A, W, b$ to make back propogation more efficient

```{r linear forward propogation}

linear_fwd <- function(A, W, b) {
  
  #A -- input (activations) from previous layer (or input data): (size of previous layer, number of examples)
  #W -- weights matrix of shape (size of current layer, size of previous layer)
  #b -- bias vector of shape (size of the current layer, 1)
  
  #creating matrix Z of shape (size of previous layer, number of examples) (linear component of NN)
  Z <- matrix((W %*% A) + b, dim(W)[1], dim(A)[2])
  
  #saving cache to make back propogation more efficient
  cache <- list(A, W, b)
  
  #saving Z and cache into a list as R does not allow functions to return multiple variables
  Z_and_cache <- list("Z" = Z,
                 "cache" = cache)
  
  return(Z_and_cache)
  
}

```

### Sigmoid and ReLU Functions
These functions take input $Z$, perform a non-linear "activation" function, and return a list containing: 

* A --- the activation values
* cache --- the input $Z$ stored to compute back propogation efficiently

The sigmoid function is $\sigma(Z) = \sigma(W A + b) = \frac{1}{ 1 + e^{-(W A + b)}}$

The Rectified Linear Unit (ReLU) function is $A = RELU(Z) = max(0, Z)$

```{r sigmoid and relu functions}

sigmoid <- function(Z) {
  
  #sigmoid function
  sig <- 1/(1+exp(-Z))
  
  #saving cache to make backpropogation more efficient
  sig_and_cache <- list("A" = sig,
                        "cache" = Z)
  
  return(sig_and_cache)
  
}

relu_func <- function(Z) {
  
  #rectified linear unit function
  relu <- pmax(Z, 0) 
  
  #saving cache to make backpropogation more efficient
  relu_and_cache <- list("A" = relu,
                         "cache" = Z)
  
  return(relu_and_cache)
  
}

```

### Linear Activation Forward
This function combines the Linear Forward Propogation function and the activation functions (sigmoid and ReLU) and returns the list *A_and_cache* containing:  

* A --- output of the activation function
* linear_cache --- cache from the linear forward step
* activation_cache --- cache from the activation function

The equation to produce $A^{[l]}$ here is $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}*A^{[l-1]}+b^{[l]})$ where $g$ is either the sigmoid or ReLU activation function.

```{r linear-activation forward propogation}

linear_activation_fwd <- function(A_prev, W, b, activation) {
  
  #applying linear and sigmoid/relu functions to activations
  if(activation == "sigmoid") {
    Z_and_cache <- linear_fwd(A_prev, W, b)
    A_and_cache <- sigmoid(Z_and_cache[["Z"]])
    } else if(activation == "relu") {
      Z_and_cache <- linear_fwd(A_prev, W, b)
      A_and_cache <- relu_func(Z_and_cache[["Z"]])
     }
  
  A_and_cache <- list("A" = A_and_cache[["A"]],
                      "linear_cache" = Z_and_cache[["cache"]],
                      "activation_cache" = A_and_cache[["cache"]])
}

```

### Cost Function
The cost function takes inputs: 

* AL --- probability matrix of label predictions of shape (n_classes, n_examples)
* Y --- true label matrix of shape (n_classes, n_examples)  

and returns the cross-entropy cost.

The cross-entropy cost $J$ is computed using the following formula: 

$J = -\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right))$

```{r cost function}

cost_func <- function(AL, Y) {
  
  #cost function
  m <- dim(Y)[2]
  logprobs <- (log(AL) * Y) + (log(1-AL) * (1-Y))
  cost <- -1/m*sum(logprobs)
  
  return(cost)
  
} 

```

### Linear Backward Function
This function takes inputs: 

* dZ --- gradient of the cost with respect to the linear output of the current layer $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$
* A_and_cache --- list containing *A_prev, W, b* from forward propogation in the current layer  

computes the derivatives $dA^{[l]}, dW^{[l]}, db^{[l]}$ and returns the list *gradients* containing:

* dA_prev --- gradient of the cost with respect to the activation of the previous layer with same shape as *A_prev*
* dW --- gradient of the cost with respect to W of current layer with same shape as *W*
* db --- gradient of the cost with respect to b of current layer with same shape as *b*

using the following formulas:  

$dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$  
$dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T}$  
$db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum\limits_{i = 1}^{m} dZ^{[l](i)}$

```{r linear backward}

linear_backward <- function(dZ, A_and_cache) {
  
  #extracting things from cache
  A_prev <- A_and_cache[[1]]
  W <- A_and_cache[[2]]
  b <- A_and_cache[[3]]
  m <- dim(A_prev)[2]
  
  #taking derivitives
  dW <- 1/m*(dZ %*% t(A_prev))
  db <- 1/m*sum(dZ)
  dA_prev <- t(W) %*% dZ

  #saving gradients  
  gradients <- list("dA_prev" = dA_prev,
                    "dW" = dW,
                    "db" = db)
  
  return(gradients)
  
}

```

### Activation Derivatives
These functions compute the derivatives of the activation functions with inputs: 

* dA --- post-activation gradient for current layer
* cache --- either linear or activation cache that was stored for backward propogation  

to return *dZ*

The computation can be represented as $dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$ where $g(.)$ is the activation function

```{r backwards sigmoid and relu}

sigmoid_backward <- function(dA, cache) {
  
  #sigmoid derivative
  Z <- cache
  s = 1/(1+exp(-Z))
  dZ = dA * s * (1-s)
  
  return(dZ)
  
}

relu_backward <- function(dA, cache) {
  
  #relu derivative
  Z <- cache
  dZ <- dA
  
  dZ[Z<=0] <- 0
  
  return(dZ)
  
}

```

### Linear Activation Backward
This function impliments back propogation for the linear->activation layer with inputs:

* dA --- post-activation gradient for current layer
* A_and_cache --- list containing linear_cache and activation_cache
* activation --- which activation to use for this layer ("sigmoid" or "relu")  

and returns the list *gradients* containing:

* dA_prev --- gradient of the cost with respect to the activation of the previous layer with same shape as *A_prev*
* dW --- gradient of the cost with respect to W of current layer with same shape as *W*
* db --- gradient of the cost with respect to b of current layer with same shape as *b*

```{r linear activation backward}

linear_activation_back <- function(dA, A_and_cache, activation) {
  
  #extracting caches
  linear_cache <- A_and_cache[["linear_cache"]]
  activation_cache <- A_and_cache[["activation_cache"]]
  
  #gradient descent
  if(activation == "relu") {
    
    dZ <- relu_backward(dA, activation_cache)
    gradients <- linear_backward(dZ, linear_cache)
    
  } else if(activation == "sigmoid") {
    
    dZ <- sigmoid_backward(dA, activation_cache)
    gradients <- linear_backward(dZ, linear_cache)
    
  }
  
  return(gradients)
  
}

```

### Updating the Parameters
This function updates the parameters of the model using gradient descent. It takes inputs: 

* parameters --- list of parameters *W1, b1, W2, b2*
* l1_gradients --- gradients from first NN layer
* l2_gradients --- gradients from second NN layer
* learning_rate --- the $\alpha$ you wish to use  

and returns the list *parameters* containing the updated parameters.

Gradient descent works using the following equations:

$W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}$  
$b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}$  

```{r updating parameters}

update_parameters <- function(parameters, l1_gradients, l2_gradients, learning_rate) {
  
  #updating parameters using learning rate and gradients
  parameters[["W1"]] <- parameters[["W1"]] - (learning_rate * l1_gradients[[2]])
  parameters[["b1"]] <- parameters[["b1"]] - (learning_rate * l1_gradients[[3]])
  parameters[["W2"]] <- parameters[["W2"]] - (learning_rate * l2_gradients[[2]])
  parameters[["b2"]] <- parameters[["b2"]] - (learning_rate * l2_gradients[[3]])
  
  return(parameters)
  
}

```

### Two Layer Model Function
This function combines all previous functions into a NN function. The inputs are:

* X --- image data from training set
* Y --- label matrix from training set
* layers_dimensions --- list containing *n_x, n_h, n_y*  

and has the tunable parameters of:

* learning_rate --- the learning rate or $\alpha$ you want to use
* num_iterations --- number of iterations you want the model to run through while training
* print_cost --- whether or not to print the cost after every 100 iterations

This returns the list *parameters* which contains the tuned parameters *W1, b1, W2, b2*

```{r two layer model}

#defining the constants
n_x <- 784
n_h <- 32
n_y <- 10

#saving constants to a list
layers_dimensions <- list("n_x" = n_x,
                          "n_h" = n_h,
                          "n_y" = n_y)

two_layer_model <- function(X, Y, layers_dimensions, learning_rate = .0075, num_iterations = 3000, print_cost = FALSE) {
  
  set.seed(123)
  m <- dim(X)[2]
  n_x <- layers_dimensions[["n_x"]]
  n_h <- layers_dimensions[["n_h"]]
  n_y <- layers_dimensions[["n_y"]]
  
  parameters <- initialize_parameters(n_x, n_h, n_y)
  
  W1 <- parameters[["W1"]]
  b1 <- parameters[["b1"]]
  W2 <- parameters[["W2"]]
  b2 <- parameters[["b2"]]
  
  for(i in 1:num_iterations) {
    
    A1_and_cache <- linear_activation_fwd(X, W1, b1, activation = "relu")
    A2_and_cache <- linear_activation_fwd(A1_and_cache[["A"]], W2, b2, activation = "sigmoid")
    
    cost <- cost_func(A2_and_cache[["A"]], Y)
    
    dA2 <- -((Y/A2_and_cache[["A"]]) - ((1-Y)/(1-A2_and_cache[["A"]])))
    
    gradients2 <- linear_activation_back(dA2, A2_and_cache, activation = "sigmoid")
    gradients1 <- linear_activation_back(gradients2[[1]], A1_and_cache, activation = "relu")
    
    parameters <- update_parameters(parameters, gradients1, gradients2, learning_rate)
    
    W1 <- parameters[["W1"]]
    b1 <- parameters[["b1"]]
    W2 <- parameters[["W2"]]
    b2 <- parameters[["b2"]]
    
    if(print_cost == TRUE & i%%100 == 0) {
      print(cost)
    }
    
    
  }
  return(parameters)
}

```

## Running the Model
Depending on the number of iterations given, this can take quite a bit of time due to the huge amount of training data we are giving it

```{r running model}

parameters <- two_layer_model(X, y, layers_dimensions, .0075, 3000, TRUE)

```

## Testing the NN
As we do not currently have a separate testing set, this bit of code just gives you the opportunity to see the predicted versus actual label for a given image in the training set was.  
It takes the inputs:

* X --- image data from training set
* Y --- label matrix from training set
* num --- number between 0 and 42000 to select an image
* parameters --- tuned list of parameters (output of the two_layer_model function)

and returns the prediction and the true label

```{r testing NN}

#function to show models prediction and the true label
predict <- function(X, Y, num, parameters) {
  
  img <- matrix(X[,num], 784, 1)
  truevec <- y[,num]
  
  W1 <- parameters[["W1"]]
  b1 <- parameters[["b1"]]
  W2 <- parameters[["W2"]]
  b2 <- parameters[["b2"]]
  
  A1 <- linear_activation_fwd(img, W1, b1, activation = "relu")
  A2 <- linear_activation_fwd(A1[["A"]], W2, b2, activation = "sigmoid")
  
  predvec <- A2[["A"]]
  pred <- which(predvec == max(predvec)) - 1
  true <- which(truevec == max(truevec)) - 1
  acc <- ifelse(pred == true, "correctly", "incorrectly")
  
  print(paste0("The Neural Network ", acc, " predicted ", pred, " and the true value is ", true))
  
}

predict(X, Y, 17000, parameters)


```