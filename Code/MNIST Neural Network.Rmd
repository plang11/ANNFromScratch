---
title: "MNISTNeuralNetwork"
author: "Patrick Lang"
date: "11/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading packages}

#onehot encoding
library(data.table) 
library(mltools)

```

```{r importing data}

train <- read.csv("../Data/train.csv")

```

```{r splitting into X and y}

#reshaping X into shape (n_pixels, n_examples), it was previously (n_examples, n_pixels)
X <- t(as.matrix(train[,-1]))/255
X <- matrix(X, 784, 42000)

#one hot encoding y as there are 10 possible classes (digits) and our output layer will contain 10 units, also reshaping to be (n_classes, n_examples)
#only time a package is used
y <- data.table(train[,1])
y$V1 <- as.factor(y$V1)
y <- one_hot(y, cols = "V1")
y <- t(as.matrix(y))
y <- matrix(y, 10, 42000)

```

```{r initializing parameters}

#creating function to initialize weights and biases of neural network
initialize_parameters <- function(n_x, n_h, n_y) {
  
  set.seed(123)
  
  #n_x - size of input layer (n_pixels)
  #n_h - size of hidden layer (n_units)
  #n_y - size of output layer (n_classes)
  
  #randomly initializing weight matrices using uniform distribution and bias vectors as zero vectors
  W1 <- matrix(runif(n_h * n_x), n_h, n_x)*.01
  b1 <- as.vector(rep(0, n_h))
  W2 <- matrix(runif(n_y * n_h), n_y, n_h)*.01
  b2 <- as.vector(rep(0, n_y))
  
  #W1 - weight matrix of shape (n_h, n_x)
  #b1 - bias vector of shape (n_h, 1)
  #W2 - weight matrix of shape (n_y, n_h)
  #b2 - bias vector of shape (n_y, 1)
  
  #combining parameters into a list
  parameters <- list("W1" = W1,
                     "b1" = b1,
                     "W2" = W2,
                     "b2" = b2)
  
  return(parameters)
  
}

```

```{r linear forward propogation}

linear_fwd <- function(A, W, b) {
  
  #A -- input (activations) from previous layer (or input data): (size of previous layer, number of examples)
  #W -- weights matrix of shape (size of current layer, size of previous layer)
  #b -- bias vector of shape (size of the current layer, 1)
  
  #creating matrix Z of shape (size of previous layer, number of examples) (linear component of NN)
  Z <- matrix((W %*% A) + b, dim(W)[1], dim(A)[2])
  
  #saving cache to make back propogation more efficient
  cache <- list(A, W, b)
  
  #saving Z and cache into a list as R does not allow functions to return multiple variables
  Z_and_cache <- list("Z" = Z,
                 "cache" = cache)
  
  return(Z_and_cache)
  
}

```

```{r sigmoid and relu functions}

sigmoid <- function(Z) {
  
  #sigmoid function
  sig <- 1/(1+exp(-Z))
  
  #saving cache to make backpropogation more efficient
  sig_and_cache <- list("A" = sig,
                        "cache" = Z)
  
  return(sig_and_cache)
  
}

relu_func <- function(Z) {
  
  #rectified linear unit function
  relu <- pmax(Z, 0) 
  
  #saving cache to make backpropogation more efficient
  relu_and_cache <- list("A" = relu,
                         "cache" = Z)
  
  return(relu_and_cache)
  
}

```

```{r linear-activation forward propogation}

linear_activation_fwd <- function(A_prev, W, b, activation) {
  
  #applying linear and sigmoid/relu functions to activations
  if(activation == "sigmoid") {
    Z_and_cache <- linear_fwd(A_prev, W, b)
    A_and_cache <- sigmoid(Z_and_cache[["Z"]])
    } else if(activation == "relu") {
      Z_and_cache <- linear_fwd(A_prev, W, b)
      A_and_cache <- relu_func(Z_and_cache[["Z"]])
     }
  
  A_and_cache <- list("A" = A_and_cache[["A"]],
                      "linear_cache" = Z_and_cache[["cache"]],
                      "activation_cache" = A_and_cache[["cache"]])
}

```

```{r cost function}

cost_func <- function(AL, Y) {
  
  #cost function
  m <- dim(Y)[2]
  logprobs <- (log(AL) * Y) + (log(1-AL) * (1-Y))
  cost <- -1/m*sum(logprobs)
  
  return(cost)
  
} 

```

```{r linear backward}

linear_backward <- function(dZ, A_and_cache) {
  
  #extracting things from cache
  A_prev <- A_and_cache[[1]]
  W <- A_and_cache[[2]]
  b <- A_and_cache[[3]]
  m <- dim(A_prev)[2]
  
  #taking derivitives
  dW <- 1/m*(dZ %*% t(A_prev))
  db <- 1/m*sum(dZ)
  dA_prev <- t(W) %*% dZ

  #saving gradients  
  gradients <- list("dA_prev" = dA_prev,
                    "dW" = dW,
                    "db" = db)
  
  return(gradients)
  
}

```

```{r backwards sigmoid and relu}

sigmoid_backward <- function(dA, cache) {
  
  #sigmoid derivative
  Z <- cache
  s = 1/(1+exp(-Z))
  dZ = dA * s * (1-s)
  
  return(dZ)
  
}

relu_backward <- function(dA, cache) {
  
  #relu derivative
  Z <- cache
  dZ <- dA
  
  dZ[Z<=0] <- 0
  
  return(dZ)
  
}

```

```{r linear activation backward}

linear_activation_back <- function(dA, A_and_cache, activation) {
  
  #extracting caches
  linear_cache <- A_and_cache[["linear_cache"]]
  activation_cache <- A_and_cache[["activation_cache"]]
  
  #gradient descent
  if(activation == "relu") {
    
    dZ <- relu_backward(dA, activation_cache)
    gradients <- linear_backward(dZ, linear_cache)
    
  } else if(activation == "sigmoid") {
    
    dZ <- sigmoid_backward(dA, activation_cache)
    gradients <- linear_backward(dZ, linear_cache)
    
  }
  
  return(gradients)
  
}

```

```{r updating parameters}

update_parameters <- function(parameters, l1_gradients, l2_gradients, learning_rate) {
  
  #updating parameters using learning rate and gradients
  parameters[["W1"]] <- parameters[["W1"]] - (learning_rate * l1_gradients[[2]])
  parameters[["b1"]] <- parameters[["b1"]] - (learning_rate * l1_gradients[[3]])
  parameters[["W2"]] <- parameters[["W2"]] - (learning_rate * l2_gradients[[2]])
  parameters[["b2"]] <- parameters[["b2"]] - (learning_rate * l2_gradients[[3]])
  
  return(parameters)
  
}

```

```{r two layer model}

#defining the constants
n_x <- 784
n_h <- 32
n_y <- 10

#saving constants to a list
layers_dimensions <- list("n_x" = n_x,
                          "n_h" = n_h,
                          "n_y" = n_y)

two_layer_model <- function(X, Y, layers_dimensions, learning_rate = .0075, num_iterations = 3000, print_cost = FALSE) {
  
  set.seed(123)
  m <- dim(X)[2]
  n_x <- layers_dimensions[["n_x"]]
  n_h <- layers_dimensions[["n_h"]]
  n_y <- layers_dimensions[["n_y"]]
  
  parameters <- initialize_parameters(n_x, n_h, n_y)
  
  W1 <- parameters[["W1"]]
  b1 <- parameters[["b1"]]
  W2 <- parameters[["W2"]]
  b2 <- parameters[["b2"]]
  
  for(i in 1:num_iterations) {
    
    A1_and_cache <- linear_activation_fwd(X, W1, b1, activation = "relu")
    A2_and_cache <- linear_activation_fwd(A1_and_cache[["A"]], W2, b2, activation = "sigmoid")
    
    cost <- cost_func(A2_and_cache[["A"]], Y)
    
    dA2 <- -((Y/A2_and_cache[["A"]]) - ((1-Y)/(1-A2_and_cache[["A"]])))
    
    gradients2 <- linear_activation_back(dA2, A2_and_cache, activation = "sigmoid")
    gradients1 <- linear_activation_back(gradients2[[1]], A1_and_cache, activation = "relu")
    
    parameters <- update_parameters(parameters, gradients1, gradients2, learning_rate)
    
    W1 <- parameters[["W1"]]
    b1 <- parameters[["b1"]]
    W2 <- parameters[["W2"]]
    b2 <- parameters[["b2"]]
    
    if(print_cost == TRUE & i%%100 == 0) {
      print(cost)
    }
    
    
  }
  return(parameters)
}

```



```{r running model}

parameters <- two_layer_model(X, y, layers_dimensions, .0075, 3000, TRUE)

```

```{r testing NN}

predict <- function(X, num, parameters) {
  
  img <- matrix(X[,num], 784, 1)
  
  W1 <- parameters[["W1"]]
  b1 <- parameters[["b1"]]
  W2 <- parameters[["W2"]]
  b2 <- parameters[["b2"]]
  
  A1 <- linear_activation_fwd(img, W1, b1, activation = "relu")
  A2 <- linear_activation_fwd(A1[["A"]], W2, b2, activation = "sigmoid")
  
  return(A2[["A"]])
  
}

test <- matrix(X[,16000], 784, 1)
test_y <- y[,16000]

prediction <- predict(X, 16000, parameters)

View(prediction)
View(test_y)

```